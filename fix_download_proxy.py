import re

with open('worker.py', 'r', encoding='utf-8') as f:
    content = f.read()

# å®Œæ•´æ›¿æ› download_file æ–¹æ³•
pattern = r'    def download_file\(self, url, save_path.*?\n(?=    def |\Z)'

replacement = '''    def download_file(self, url, save_path, max_retries=3):
        """ä¸‹è¼‰æª”æ¡ˆï¼ˆS3 ä½¿ç”¨ä»£ç†ï¼‰"""
        # S3 URL ä½¿ç”¨å¾Œç«¯ä»£ç†
        if 's3.amazonaws.com' in url or 's3-ap-northeast-1' in url:
            return self._download_via_proxy(url, save_path, max_retries)
        
        # å…¶ä»– URL ç›´æ¥ä¸‹è¼‰
        return self._download_direct(url, save_path, max_retries)
    
    def _download_via_proxy(self, url, save_path, max_retries):
        """é€šéå¾Œç«¯ä»£ç†ä¸‹è¼‰"""
        import urllib.parse
        proxy_url = f"{self.current_base}/aigen/api/proxy_s3/?url={urllib.parse.quote(url)}"
        
        logger.info(f"ğŸ“¥ ä»£ç†ä¸‹è¼‰: {url}")
        
        for attempt in range(1, max_retries + 1):
            try:
                headers = self._get_auth_headers()
                response = requests.get(proxy_url, headers=headers, timeout=120, stream=True)
                
                if response.status_code == 200:
                    total_size = int(response.headers.get('content-length', 0))
                    downloaded = 0
                    
                    with open(save_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded += len(chunk)
                    
                    logger.info(f"âœ… ä¸‹è¼‰å®Œæˆ: {save_path} ({downloaded / 1024 / 1024:.2f}MB)")
                    return True
                else:
                    logger.error(f"ä»£ç†å¤±æ•—: HTTP {response.status_code}")
                    if attempt < max_retries:
                        time.sleep(3)
                        continue
                    return False
                    
            except Exception as e:
                logger.error(f"ä»£ç†éŒ¯èª¤ ({attempt}/{max_retries}): {e}")
                if attempt < max_retries:
                    time.sleep(3)
                    continue
                return False
        
        return False
    
    def _download_direct(self, url, save_path, max_retries):
        """ç›´æ¥ä¸‹è¼‰"""
        for attempt in range(1, max_retries + 1):
            try:
                logger.info(f"ğŸ“¥ ä¸‹è¼‰: {url}")
                response = requests.get(url, timeout=300, stream=True)
                
                if response.status_code == 200:
                    with open(save_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                    
                    logger.info(f"âœ… ä¸‹è¼‰å®Œæˆ: {save_path}")
                    return True
                else:
                    logger.error(f"ä¸‹è¼‰å¤±æ•—: HTTP {response.status_code}")
                    if attempt < max_retries:
                        time.sleep(3)
                        continue
                    return False
                    
            except Exception as e:
                logger.error(f"ä¸‹è¼‰éŒ¯èª¤ ({attempt}/{max_retries}): {e}")
                if attempt < max_retries:
                    time.sleep(3)
                    continue
                return False
        
        return False

'''

content = re.sub(pattern, replacement, content, flags=re.DOTALL)

with open('worker.py', 'w', encoding='utf-8') as f:
    f.write(content)

print("âœ… å·²ä¿®æ”¹ç‚ºä»£ç†æ¨¡å¼")
