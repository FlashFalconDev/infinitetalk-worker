def generate(args):
    rank = int(os.getenv("RANK", 0))
    world_size = int(os.getenv("WORLD_SIZE", 1))
    local_rank = int(os.getenv("LOCAL_RANK", 0))
    device = local_rank
    _init_logging(rank)

    if args.offload_model is None:
        args.offload_model = False if world_size > 1 else True
        logging.info(
            f"offload_model is not specified, set to {args.offload_model}.")
    if world_size > 1:
        torch.cuda.set_device(local_rank)
        dist.init_process_group(
            backend="nccl",
            init_method="env://",
            rank=rank,
            world_size=world_size)
    else:
        assert not (
            args.t5_fsdp or args.dit_fsdp
        ), f"t5_fsdp and dit_fsdp are not supported in non-distributed environments."
        assert not (
            args.ulysses_size > 1 or args.ring_size > 1
        ), f"context parallel are not supported in non-distributed environments."

    if args.ulysses_size > 1 or args.ring_size > 1:
        assert args.ulysses_size * args.ring_size == world_size, f"The number of ulysses_size and ring_size should be equal to the world size."
        from xfuser.core.distributed import (
            init_distributed_environment,
            initialize_model_parallel,
        )
        init_distributed_environment(
            rank=dist.get_rank(), world_size=dist.get_world_size())

        initialize_model_parallel(
            sequence_parallel_degree=dist.get_world_size(),
            ring_degree=args.ring_size,
            ulysses_degree=args.ulysses_size,
        )

    # TODO: use prompt refine
    # if args.use_prompt_extend:
    #     if args.prompt_extend_method == "dashscope":
    #         prompt_expander = DashScopePromptExpander(
    #             model_name=args.prompt_extend_model,
    #             is_vl="i2v" in args.task or "flf2v" in args.task)
    #     elif args.prompt_extend_method == "local_qwen":
    #         prompt_expander = QwenPromptExpander(
    #             model_name=args.prompt_extend_model,
    #             is_vl="i2v" in args.task,
    #             device=rank)
    #     else:
    #         raise NotImplementedError(
    #             f"Unsupport prompt_extend_method: {args.prompt_extend_method}")

    cfg = WAN_CONFIGS[args.task]
    if args.ulysses_size > 1:
        assert cfg.num_heads % args.ulysses_size == 0, f"`{cfg.num_heads=}` cannot be divided evenly by `{args.ulysses_size=}`."

    logging.info(f"Generation job args: {args}")
    logging.info(f"Generation model config: {cfg}")

    if dist.is_initialized():
        base_seed = [args.base_seed] if rank == 0 else [None]
        dist.broadcast_object_list(base_seed, src=0)
        args.base_seed = base_seed[0]

    assert args.task == "infinitetalk-14B", 'You should choose infinitetalk in args.task.'
    

    logging.info("Creating infinitetalk pipeline.")
    wan_i2v = wan.InfiniteTalkPipeline(
        config=cfg,
        checkpoint_dir=args.ckpt_dir,
        quant_dir=args.quant_dir,
        device_id=device,
        rank=rank,
        t5_fsdp=args.t5_fsdp,
        dit_fsdp=args.dit_fsdp, 
        use_usp=(args.ulysses_size > 1 or args.ring_size > 1),  
        t5_cpu=args.t5_cpu,
        lora_dir=args.lora_dir,
        lora_scales=args.lora_scale,
        quant=args.quant,
        dit_path=args.dit_path,
        infinitetalk_dir=args.infinitetalk_dir
    )
    if args.num_persistent_param_in_dit is not None:
        wan_i2v.vram_management = True
        wan_i2v.enable_vram_management(
            num_persistent_param_in_dit=args.num_persistent_param_in_dit
        )
    
    generated_list = []
    with open(args.input_json, 'r', encoding='utf-8') as f:
        input_data = json.load(f)
        
    wav2vec_feature_extractor, audio_encoder= custom_init('cpu', args.wav2vec_dir)
    args.audio_save_dir = os.path.join(args.audio_save_dir, input_data['cond_video'].split('/')[-1].split('.')[0])
    os.makedirs(args.audio_save_dir,exist_ok=True)
    
    conds_list = []

    if args.scene_seg and is_video(input_data['cond_video']):
        time_list, cond_list = shot_detect(input_data['cond_video'], args.audio_save_dir)
        if len(time_list)==0:
            conds_list.append([input_data['cond_video']])
            conds_list.append([input_data['cond_audio']['person1']])
            if len(input_data['cond_audio'])==2:
                conds_list.append([input_data['cond_audio']['person2']])
        else:
            audio1_list = split_wav_librosa(input_data['cond_audio']['person1'], time_list, args.audio_save_dir)
            conds_list.append(cond_list)
            conds_list.append(audio1_list)
            if len(input_data['cond_audio'])==2:
                audio2_list = split_wav_librosa(input_data['cond_audio']['person2'], time_list, args.audio_save_dir)
                conds_list.append(audio2_list)
    else:
        conds_list.append([input_data['cond_video']])
        conds_list.append([input_data['cond_audio']['person1']])
        if len(input_data['cond_audio'])==2:
            conds_list.append([input_data['cond_audio']['person2']])

    if len(input_data['cond_audio'])==2:
        new_human_speech1, new_human_speech2, sum_human_speechs = audio_prepare_multi(input_data['cond_audio']['person1'], input_data['cond_audio']['person2'], input_data['audio_type'])
        sum_audio = os.path.join(args.audio_save_dir, 'sum_all.wav')
        sf.write(sum_audio, sum_human_speechs, 16000)
        input_data['video_audio'] = sum_audio
    else:
        human_speech = audio_prepare_single(input_data['cond_audio']['person1'])
        sum_audio = os.path.join(args.audio_save_dir, 'sum_all.wav')
        sf.write(sum_audio, human_speech, 16000)
        input_data['video_audio'] = sum_audio
    logging.info("Generating video ...")
        
    for idx, items in enumerate(zip(*conds_list)):
        print(items)
        input_clip = {}
        input_clip['prompt'] = input_data['prompt']
        input_clip['cond_video'] = items[0]

        if 'audio_type' in input_data:
            input_clip['audio_type'] = input_data['audio_type']
        if 'bbox' in input_data:
            input_clip['bbox'] = input_data['bbox']
        cond_audio = {}
        if args.audio_mode=='localfile':
            if len(input_data['cond_audio'])==2:
                new_human_speech1, new_human_speech2, sum_human_speechs = audio_prepare_multi(items[1], items[2], input_data['audio_type'])
                audio_embedding_1 = get_embedding(new_human_speech1, wav2vec_feature_extractor, audio_encoder)
                audio_embedding_2 = get_embedding(new_human_speech2, wav2vec_feature_extractor, audio_encoder)
                emb1_path = os.path.join(args.audio_save_dir, '1.pt')
                emb2_path = os.path.join(args.audio_save_dir, '2.pt')
                sum_audio = os.path.join(args.audio_save_dir, 'sum.wav')
                sf.write(sum_audio, sum_human_speechs, 16000)
                torch.save(audio_embedding_1, emb1_path)
                torch.save(audio_embedding_2, emb2_path)
                cond_audio['person1'] = emb1_path
                cond_audio['person2'] = emb2_path
                input_clip['video_audio'] = sum_audio
                v_length = audio_embedding_1.shape[0]
            elif len(input_data['cond_audio'])==1:
                human_speech = audio_prepare_single(items[1])
                audio_embedding = get_embedding(human_speech, wav2vec_feature_extractor, audio_encoder)
                emb_path = os.path.join(args.audio_save_dir, '1.pt')
                sum_audio = os.path.join(args.audio_save_dir, 'sum.wav')
                sf.write(sum_audio, human_speech, 16000)
                torch.save(audio_embedding, emb_path)
                cond_audio['person1'] = emb_path
                input_clip['video_audio'] = sum_audio
                v_length = audio_embedding.shape[0]
        
        input_clip['cond_audio'] = cond_audio
                    
        video = wan_i2v.generate_infinitetalk(
            input_clip,
            size_buckget=args.size,
            motion_frame=args.motion_frame,
            frame_num=args.frame_num,
            shift=args.sample_shift,
            sampling_steps=args.sample_steps,
            text_guide_scale=args.sample_text_guide_scale,
            audio_guide_scale=args.sample_audio_guide_scale,
            seed=args.base_seed,
            offload_model=args.offload_model,
            max_frames_num=args.frame_num if args.mode == 'clip' else args.max_frame_num,
            color_correction_strength = args.color_correction_strength,
            extra_args=args,
            )
        
        generated_list.append(video)

    if rank == 0:
        
        if args.save_file is None:
            formatted_time = datetime.now().strftime("%Y%m%d_%H%M%S")
            formatted_prompt = input_clip['prompt'].replace(" ", "_").replace("/",
                                                                        "_")[:50]
            args.save_file = f"{args.task}_{args.size.replace('*','x') if sys.platform=='win32' else args.size}_{args.ulysses_size}_{args.ring_size}_{formatted_prompt}_{formatted_time}"
        
        sum_video = torch.cat(generated_list, dim=1)
        save_video_ffmpeg(sum_video, args.save_file, [input_data['video_audio']], high_quality_save=False)
   
    logging.info(f"Saving generated video to {args.save_file}.mp4")  
    logging.info("Finished.")


if __name__ == "__main__":
    args = _parse_args()
    generate(args)
